{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G1useppe/ma5851_capstone_T0CE/blob/main/WEBCRAWLER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Domains"
      ],
      "metadata": {
        "id": "G5Nmwuvirr2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.1 Target"
      ],
      "metadata": {
        "id": "w_fg3af4r0bh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The webscraper is intended to target Indeed Australia at au.indeed.com. Due to the fact that most employers cross-post to many job board services, only one target was chosen to avoid duplication. Indeed was chosen over industry competitors, Seek and Jora for two primary reasons. The first of these is that in preliminary investigations, it was found that Indeed will return up to 915 advertisements on a single search, whereas Seek and Jora would only provide 500 and 600 respectively. Secondly, Indeed has standardized fields where advertisers are coerced to provide sensible values. The best example of this is that Indeed forces advertisers to declare salary as either a fixed amount or a range, and then accompany this with a pay frequency (an hour, a day, a year). Seek and Jora's negligence to uphold some sort of standardization allowed values such as 'Competitive Salary' to be considered appropriate values for the Salary field. "
      ],
      "metadata": {
        "id": "MdAp2HRRuOpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.2 Domain Limitations"
      ],
      "metadata": {
        "id": "at4ypU_LsAxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary limitation faced in the Indeed domain is the low proportion of job postings that had an attached salary. Whilst the non-salaried data has bias assessment applications, there is still a lot of data that will not be beneficial to the machine learning pipeline. Furthermore, not all jobs get listed on advertising boards in the contemporary environment of professional social networking (LinkedIn etc.), so whilst the web scraper should provide a sound picture of the employment market for Data Scientists, there may be some gaps in the overall picture."
      ],
      "metadata": {
        "id": "IAwAQi0Rcwqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.3 Data Alignment"
      ],
      "metadata": {
        "id": "0keukcHpsJS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data aligned to our investigation to be extracted is as follows:\n",
        "1.   Salary. This is intended to be the target variable for the machine learning component.\n",
        "2.   Location data (State, City). This is intended to capture the variability represented by the location in which people work. It is expected that economic hubs such as Melbourne and Sydney attract higher salaries due to the associated cost of living.\n",
        "3.  Text data (Job Title, Company, Description). This data will form the nulk of the feature set following the data wrangling and feature extraction. It is expected that most of the signal in the dataset will come from these features, particularly those extracted from the Description field, which is expected to contain a rich corpus. "
      ],
      "metadata": {
        "id": "5wN1CyybdnBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.4 Copyright and Legal Considerations"
      ],
      "metadata": {
        "id": "vaFeDsvcsR4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several lines of communication were opened with Indeed to seek express approval to conduct this work. Unofortunately, departments were reluctant to approach the request and, in most cases deferred the query to another department. A firm resolution on the matter could not be achieved in the time frame for this work. There is a sizeable body of work to do with data collection from Indeed among non-academic data science communitites to suggest that there is no active motion from Indeed to suppress projects that do not make a commerical gain. This sentiment is in line with the Indeed Terms of Use."
      ],
      "metadata": {
        "id": "3F_IHGBN0gMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Webcrawler Workflow"
      ],
      "metadata": {
        "id": "CzNf7bnirtFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.1 Technology Components"
      ],
      "metadata": {
        "id": "XVzcsDXVsZvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The technology used for the web scraper is Selenium (more specifically selenium.webdriver), first released by author Baiju Muthukadan in 2011. It is primarily used for automating web applications and in this work, this ability is harnessed by pairing selenium with a Google Chrome driver. This combination provides the capability to complete tasks as a human would, but with much greater speed due to automation. The tasks Selenium performs in this work are:\n",
        "\n",
        "*   Clicking buttons\n",
        "*   Passing information to input fields\n",
        "*   Searching for HTML elements\n",
        "*   Scraping these elements into a database\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BgyecUUvjNjZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBMydG3ore0t"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.2-3 Domain Complexity and Methodologies"
      ],
      "metadata": {
        "id": "B0Y95R-qmkPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The workflow contains three major processes, and these have separate domain complexities. The first of these is to generate a search for Data Science postings. A consistent aspect of the Indeed HTML architecture is the provision of unique class names, which lowers the domain complexity and makes elements easy to locate. In the case of generating the search, the required elements are easily found due to the HTML coding practices.\n",
        "The beauty of Selenium is that it can be used to mirror human workflows, and many functions are based on these primitive browser actions. To generate the search, two actions must be performed.\n",
        "\n",
        "1.   Pass the search term to the keywords field\n",
        "2.   Click the search button\n",
        "\n",
        "To complete these processes, the element must be found, and the action must be ordered. Selenium has a range of locator methods to be used in conjunction with find_element(), and the predominant method in this work is By.XPATH, where the element type, and an element feature are passed to find matching elements in the HTML. In this case, arguments are generated to find the required elements. send_keys() and click() are passed to the discovered elements to complete\n",
        "\n"
      ],
      "metadata": {
        "id": "eNg7x5xu3GyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code block implements the find_elements() function from selenium.webdriver, where all instances of the search are returned. In this case, the search parameter is By.XPATH, a versatile and powerful locator. The element type ('a'), and the element class ('jcs-JobTitle css-jspxzf eu4oa1w0') are passed to the function, matching values are returned. Using list comprehension, these values can be added to the master list for later querying. This method does pick up some URLs that don't correspond to job listings, and these are filtered in the following code block. "
      ],
      "metadata": {
        "id": "Lx3-wkGk_Tbj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH6kHPNjre0u"
      },
      "outputs": [],
      "source": [
        "DRIVER_PATH = 'C:\\Program Files (x86)\\chromedriver.exe'\n",
        "driver = webdriver.Chrome(DRIVER_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvVjr3Z6re0v"
      },
      "outputs": [],
      "source": [
        "base_url = 'http://au.indeed.com'\n",
        "driver.get(base_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67GcoUmPre0w"
      },
      "outputs": [],
      "source": [
        "keywords_field_arg = '//input[@class=\"icl-TextInput-control icl-TextInput-control--withIconRight\"]'\n",
        "keywords_field = driver.find_element(By.XPATH, keywords_field_arg)\n",
        "keywords_field.send_keys(\"Data Science\")\n",
        "\n",
        "initial_search_button = driver.find_element(By.XPATH, \"//button[normalize-space()='Find jobs']\")\n",
        "initial_search_button.click()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second process is to collate a group of URLs corresponding to the jobs listings to later query. Much like the first process, the domain complexity is mitigated by the fact that each job listing URL is assigned the same class.\n",
        "\n",
        "The below code blocks implement the find_elements() function from selenium.webdriver, where all instances of the search are returned. Instead of actioning any of these URLs, the intention is to collect the data encased in the element by calling get_attribute(). The helper function to drive the webscraper to the next page once it has exhausted the element finding uses the By.CSS_SELECTOR as the locator method, which behaves slightly differently to By.XPATH, but accepts a similar argument to target some elements attributes not captured by XPATH. To locate the URLs, By.XPATH is employed. The element type ('a'), and the element class ('jcs-JobTitle css-jspxzf eu4oa1w0') are passed to the function, matching values are returned. Using list comprehension, these values can be added to the master list for later querying. This method does pick up the link for the sign in button on each of the 60 pages, however these are filtered in the subsequent code block. This method was evaluated to be optimal as it only makes one call for data collection to the HTML code per page."
      ],
      "metadata": {
        "id": "R0Gep2xcAmoO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjZ0IclTre0w"
      },
      "outputs": [],
      "source": [
        "def next_page():\n",
        "    next_page_url = driver.find_element(By.CSS_SELECTOR, \"a[aria-label='Next']\").get_attribute('href')\n",
        "    driver.get(next_page_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5w1DXrrre0w"
      },
      "outputs": [],
      "source": [
        "job_urls = []\n",
        "for page in range(60):\n",
        "    elems = driver.find_elements(By.XPATH, \"//a[@class='jcs-JobTitle css-jspxzf eu4oa1w0']\")\n",
        "    links = [elem.get_attribute('href') for elem in elems]\n",
        "    for link in links:\n",
        "        job_urls.append(link)\n",
        "    next_page()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MZLAEeWre0w",
        "outputId": "d9191f8b-aa2a-4fe8-f801-2508d11e3155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "826"
            ]
          },
          "execution_count": 217,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for url in job_urls:\n",
        "    if url[0:27] != 'https://au.indeed.com/rc/cl':\n",
        "        job_urls.remove(url)\n",
        "\n",
        "len(job_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmpVZDKbre0x",
        "outputId": "1af89470-5277-4a98-f535-286c06836a5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'http://www.afl.com.au'"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def drop_https(url):\n",
        "    return url[0:4]+url[5:] #return all but the fifth character\n",
        "drop_https(\"https://www.afl.com.au\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The third process is the individual querying of the HTML in each URL obtained in the previous step. This process is a better case to explain the complexity of the domain, which was found to be very approachable due to the HTML coding practices in place. The concept that makes Indeed relatively easy to scrape is the consistent provision of classes to elements in the code. \n",
        "\n",
        "The only domain complexity related challenge was encountered in this step. This was initial difficulty in determining a vector to separate the company and city data, as the class was the same for both containers. Eventually, the class for the container in which both pieces of data was encapsulated, and the issue can easily be mitigated in the data wrangling component of this work."
      ],
      "metadata": {
        "id": "KMWLqGJG45od"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bPwl2rxre0x"
      },
      "outputs": [],
      "source": [
        "job_title_arg = \"//h1[@class='icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title']\"\n",
        "company_city_arg = \"//div[@class='icl-u-xs-mt--xs icl-u-textColor--secondary jobsearch-JobInfoHeader-subtitle jobsearch-DesktopStickyContainer-subtitle']\"\n",
        "salary_arg = \"//span[@class='icl-u-xs-mr--xs']\"\n",
        "description_arg = \"//div[@class='jobsearch-jobDescriptionText']\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1N2rYmXre0x"
      },
      "outputs": [],
      "source": [
        "unstructured_data = []\n",
        "for url in job_urls:\n",
        "    record = [] #start a blank record\n",
        "    driver.get(drop_https(url))\n",
        "    try: #to get the company and city\n",
        "        company_city = driver.find_element(By.XPATH, company_city_arg).get_attribute('innerText')\n",
        "        record.append(company_city)\n",
        "    except:\n",
        "        record.append(np.nan)\n",
        "    try: #to get the job title\n",
        "        job_title = driver.find_element(By.XPATH, job_title_arg).get_attribute('innerText')\n",
        "        record.append(job_title)\n",
        "    except:\n",
        "        record.append(np.nan)\n",
        "    try: #to get salary\n",
        "        salary = driver.find_element(By.XPATH, salary_arg).get_attribute('innerText')\n",
        "        record.append(salary)\n",
        "    except:\n",
        "        record.append(np.nan)   \n",
        "    try: #to get description\n",
        "        description = driver.find_element(By.XPATH, description_arg).get_attribute('innerText')\n",
        "        record.append(description)\n",
        "    except:\n",
        "        record.append(np.nan)   \n",
        "        \n",
        "    unstructured_data.append(record)    \n",
        "\n",
        "str_data = pd.DataFrame(unstructured_data, columns = ['Company_City', 'Job Title', 'Salary', 'Description'])\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.4 Data Storage"
      ],
      "metadata": {
        "id": "0s31ND8oDU4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As demonstrated in the above code block, a list of lists was created to store the data primitively, and this was converted to a pandas data frame."
      ],
      "metadata": {
        "id": "Pk40iDNTCY61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output"
      ],
      "metadata": {
        "id": "wr0GJRkhYed1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHwZJyG4re0y"
      },
      "outputs": [],
      "source": [
        "str_data.to_excel(\"output.xlsx\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpointing served as an essential procedure in this work given the large scope of the project. Converting the data to an Excel file allowed a robust, offline copy of the generate data to both protect the progress made, and to pass the data into a different workbook for the data processing component."
      ],
      "metadata": {
        "id": "luPH7Yc3C5OT"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "WEBCRAWLER.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
